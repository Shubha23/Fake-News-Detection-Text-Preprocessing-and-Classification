{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The notebook accomplishes these tasks -**\n* Text cleaning and preprocessing of fake_or_real_news dataset using NLTK and Regex library.\n* Creating and transforming clean text into tf-idf vectors.\n* Learning models like Passive Aggressive Classifier, XGBoost and LGBM to perform classification of fake and real news. (A few other algorithms were also tried but only best three out of those are chosen.)\n* Evaluate each model's performance based on the accuracy scores and confusion matrices they produced.\n\n**The following text preprocessing steps are performed here -**\n* Remove punctuations\n* Convert text to tokens\n* Remove tokens of length less than or equal to 3\n* Remove stopwords using NLTK corpus stopwords list to match\n* Apply lemmatization"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport string as st\nimport re\nimport nltk\nfrom nltk import PorterStemmer, WordNetLemmatizer\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how the labels are distributed\nprint(np.unique(data['label']))\nprint(np.unique(data['label'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text cleaning and processing steps-**\n\n* Remove punctuations\n* Convert text to tokens\n* Remove tokens of length less than or equal to 3\n* Remove stopwords using NLTK corpus stopwords list to match\n* Apply lemmatization\n* Convert words to feature vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all punctuations from the text\n\ndef remove_punct(text):\n    return (\"\".join([ch for ch in text if ch not in st.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['removed_punc'] = data['text'].apply(lambda x: remove_punct(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n    on special characters, tabs or any other string based on which text is to be seperated into tokens.\n'''\ndef tokenize(text):\n    text = re.split('\\s+' ,text)\n    return [x.lower() for x in text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tokens'] = data['removed_punc'].apply(lambda msg : tokenize(msg))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove tokens of length less than 3\ndef remove_small_words(text):\n    return [x for x in text if len(x) > 3 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['filtered_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n    list could be created and used to limit the matches in input text. \n'''\ndef remove_stopwords(text):\n    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['clean_tokens'] = data['filtered_tokens'].apply(lambda x : remove_stopwords(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply lemmatization on tokens\ndef lemmatize(text):\n    word_net = WordNetLemmatizer()\n    return [word_net.lemmatize(word) for word in text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['lemma_words'] = data['clean_tokens'].apply(lambda x : lemmatize(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create sentences to get clean text as input for vectors\n\ndef return_sentences(tokens):\n    return \" \".join([word for word in tokens])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['clean_text'] = data['lemma_words'].apply(lambda x : return_sentences(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a basic word cloud \nfrom wordcloud import WordCloud, ImageColorGenerator\n\ntext = \" \".join([x for x in data['clean_text']])\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=30, max_words=1000).generate(text)\n\n# Display the generated image:\nplt.figure(figsize= [20,10])\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data for the model. Convert label in to binary\n\ndata['label'] = [1 if x == 'FAKE' else 0 for x in data['label']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset\n\nX_train,X_test,y_train,y_test = train_test_split(data['clean_text'], data['label'], test_size=0.2, random_state = 5)\n\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF : Term Frequency - Inverse Document Frequency**\n\nThe term frequency is the number of times a term occurs in a document. Inverse document frequency is an inverse function of the number of documents in which that a given word occurs.\n\nThe product of these two terms gives tf-idf weight for a word in the corpus. The higher the frequency of occurrence of a word, lower is it's weight and vice-versa. This gives more weightage to rare terms in the corpus and penalizes more commonly occuring terms.\n\nOther widely used vectorizer is Count vectorizer which only considers the frequency of occurrence of a word across the corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\ntfidf_train = tfidf.fit_transform(X_train)\ntfidf_test = tfidf.transform(X_test)\n\nprint(tfidf_train.toarray())\nprint(tfidf_train.shape)\nprint(tfidf_test.toarray())\nprint(tfidf_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Passive Aggressive Classifiers**\n\n* Passive Aggressive algorithms are online learning algorithms. Such an algorithm remains passive for a correct classification outcome, and turns aggressive in the event of a miscalculation, updating and adjusting.\n* Passive: If the prediction is correct, keep the model and do not make any changes. i.e., the data in the example is not enough to cause any changes in the model. \n* Aggressive: If the prediction is incorrect, make changes to the model. i.e., some change to the model may correct it.\n\nThese are typically used for large datasets where batch learning is not possible due to huge volumes of frequently incoming data.\n\n* Some important parameters -\n* C : This is the regularization parameter, and denotes the penalization the model will make on an incorrect prediction\n* max_iter : The maximum number of iterations the model makes over the training data.\n* tol : The stopping criterion.\n\nFor more details please refer to description source - https://www.geeksforgeeks.org/passive-aggressive-classifiers/\n\nWe are using a simple implementation of this model here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Passive Aggresive Classifier\npac = PassiveAggressiveClassifier(max_iter=50)\npac.fit(tfidf_train,y_train)\n\npred = pac.predict(tfidf_test)\nprint(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter = 500)\nlr.fit(tfidf_train, y_train)\nprint('Logistic Regression model fitted..')\n\npred = lr.predict(tfidf_test)\nprint(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression could not outperform XGBoost and LGBM but its performance is considerably close to them and it is much less complex."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(tfidf_train, y_train)\n\nprint('XGBoost Classifier model fitted..')\npred = xgb.predict(tfidf_test)\nprint(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\nlgbm.fit(tfidf_train, y_train)\n\nprint('LightGBM Classifier model fitted..')\npred = lgbm.predict(tfidf_test)\nprint(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}